{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832",
      "metadata": {
        "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832"
      },
      "source": [
        "## Assignment 3: $k$ Nearest Neighbor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9212c0",
      "metadata": {
        "id": "5d9212c0"
      },
      "source": [
        "**Q1.**\n",
        "1. What is the difference between regression and classification?\n",
        "2. What is a confusion table? What does it help us understand about a model's performance?\n",
        "3. What does the SSE quantify about a particular model?\n",
        "4. What are overfitting and underfitting?\n",
        "5. Why does splitting the data into training and testing sets, and choosing $k$ by evaluating accuracy or SSE on the test set, improve model performance?\n",
        "6. With classification, we can report a class label as a prediction or a probability distribution over class labels. Please explain the strengths and weaknesses of each approach."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 Answers**\n",
        "1) Regression is prediciting a numerical outcome given an observation's variables, whereas classification is prediciting a categorical outcome given an observation's variables.\n",
        "2) A confusion table is a cross-tabitulation of the true values of a dataset, and the predicited values based on the explanatory variables. It shows how accurate the model is, and in which true/predicited values it is particularly accurate or inaccurate\n",
        "3) SSE Quanitfies how inaccurate a model is. By squaring the error term, we ensure that positive and negative errors do not cancel out, and each error is included in the sum. It punishes high errors by the nature of squaring them.\n",
        "4) Overfitting is when you attribute too much value on the training data, and apply features of the sample to the model that would be unlikely to appear in further samples or the population-levle data. Underfitting is when your model is too simple to predict the sort of phenomenon it is trying to.\n",
        "5) Splitting the data and choosing *k* by testing the model on the test improves the performence of the model by avoiding overfitting based on the training data and ensuring you are making a data-driven choice on *k* instead of trying to use intuition and guess the best *k*\n",
        "6.\n",
        "Prediction-\n",
        "\n",
        "  Pros: More simple and easier to do further analysis on\n",
        "\n",
        "  Cons: Less accurate and indicative of what may actually occur, Doesn't account for classification possibilites that aren't the most likely. Can lead to bias (ie if every prediction is 51% A and 49% B, classification would show that every prediction is A)\n",
        "\n",
        "\n",
        "Probability Distributions:\n",
        "\n",
        "Pros: More accurate to what the model is actually predicting. Can create expected values of # of apperances of each classification.\n",
        "\n",
        "Cons: More complex to understand and do further work on."
      ],
      "metadata": {
        "id": "J3yT5iyOmrv5"
      },
      "id": "J3yT5iyOmrv5"
    },
    {
      "cell_type": "markdown",
      "id": "194455fa",
      "metadata": {
        "id": "194455fa"
      },
      "source": [
        "**Q2.** This question is a case study for $k$ nearest neighbor regression, using the `USA_cars_datasets.csv` data.\n",
        "\n",
        "The target variable `y` is `price` and the features are `year` and `mileage`.\n",
        "\n",
        "1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.\n",
        "2. Maxmin normalize `year` and `mileage`.\n",
        "3. Split the sample into ~80% for training and ~20% for evaluation.\n",
        "4. Use the $k$NN algorithm and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?\n",
        "5. Determine the optimal $k$ for these data.\n",
        "6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287b8586",
      "metadata": {
        "id": "287b8586"
      },
      "source": [
        "**Q3.** This is a case study on $k$ nearest neighbor classification, using the `animals.csv` data.\n",
        "\n",
        "The data consist of a label, `class`, taking integer values 1 to 7, the name of the species, `animal`, and 16 characteristics of the animal, including `hair`, `feathers`, `milk`, `eggs`, `airborne`, and so on.\n",
        "\n",
        "1. Load the data. For each of the seven class labels, print the values in the class and get a sense of what is included in that group. Perform some other EDA: How big are the classes? How much variation is there in each of the features/covariates? Which variables do you think will best predict which class?\n",
        "2. Split the data 50/50 into training and test/validation sets. (The smaller the data are, the more equal the split should be, in my experience: Otherwise, all of the members of one class end up in the training or test data, and the model falls apart.)\n",
        "3. Using all of the variables, build a $k$-NN classifier. Explain how you select $k$.\n",
        "4. Print a confusion table for the optimal model, comparing predicted and actual class label on the test set. How accurate it is? Can you interpret why mistakes are made across groups?\n",
        "5. Use only `milk`, `aquatic`, and `airborne` to train a new $k$-NN classifier. Print your confusion table. Mine does not predict all of the classes, only a subset of them. To see the underlying probabilities, use `model.predict_proba(X_test.values)` to predict probabilities rather than labels for your `X_test` test data for your fitted `model`. Are all of the classes represented? Explain your results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea89b847",
      "metadata": {
        "id": "ea89b847"
      },
      "source": [
        "**Q4.** Write your own function to make a kernel density plot.\n",
        "\n",
        "- The user should pass in a Pandas series or Numpy array.\n",
        "- The default kernel should be Gaussian, but include the uniform/bump and Epanechnikov as alternatives.\n",
        "- The default bandwidth should be the Silverman plug-in, but allow the user to specify an alternative.\n",
        "- You can use Matplotlib or Seaborn's `.lineplot`, but not an existing function that creates kernel density plots.\n",
        "\n",
        "You will have to make a lot of choices and experiment with getting errors. Embrace the challenge and track your choices in the comments in your code.\n",
        "\n",
        "Use a data set from class to show that your function works, and compare it with the Seaborn `kdeplot`.\n",
        "\n",
        "We covered the Gaussian,\n",
        "$$\n",
        "k(z) = \\dfrac{1}{\\sqrt{2\\pi}}e^{-z^2/2}\n",
        "$$\n",
        "and uniform\n",
        "$$\n",
        "k(z) = \\begin{cases}\n",
        "\\frac{1}{2}, & |z| \\le 1 \\\\\n",
        "0, & |z|>1\n",
        "\\end{cases}\n",
        "$$\n",
        "kernels in class, but the Epanechnikov kernel is\n",
        "$$\n",
        "k(z) = \\begin{cases}\n",
        "\\frac{3}{4} (1-z^2), & |z| \\le 1 \\\\\n",
        "0, & |z|>1.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "In order to make your code run reasonably quickly, consider using the `pdist` or `cdist` functions from SciPy to make distance calculations for arrays of points. The other leading alternative is to thoughtfully use NumPy's broadcasting features. Writing `for` loops will be slow, but that's fine."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}